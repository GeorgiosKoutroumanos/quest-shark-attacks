# -*- coding: utf-8 -*-
"""Shark_Attacks-Jaws.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVfo3N8ofcH9Fle6k9idmBWGGe6AWpp4
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install xlrd
!pip install fuzzywuzzy python-Levenshtein
import pandas as pd
import numpy as np
import re
from fuzzywuzzy import fuzz
# Load the Excel file

shark_attacks = pd.read_excel('/content/gdrive/MyDrive/GSAF5.xlsx')
shark_attacks.head()

"""# New Section"""

shark_attacks.columns = shark_attacks.columns.str.strip()
shark_attacks = shark_attacks.drop(columns=['pdf',
 'href formula',
 'href',
 'Case Number',
 'Case Number.1',
 'original order',
 'Unnamed: 21',
 'Unnamed: 22'])
shark_attacks.columns.tolist() #dropping not relevant columns

#clean country column

shark_attacks['Country'] = shark_attacks['Country'].str.replace(r'[^a-zA-Z0-9\s]', '', regex=True).str.upper()
shark_attacks['Country'] = shark_attacks['Country'].str.strip().fillna('Unknown') # replacing nan with 'Unknown'

# Create a dictionary to store similar country pairs
similar_countries_dict = {}

# Get unique countries to reduce redundant comparisons/ code was too slow without
unique_countries = shark_attacks['Country'].drop_duplicates()

# Find similar country names and add them to the dictionary
for i, country1 in enumerate(unique_countries):
    for j, country2 in enumerate(unique_countries):
        if i < j:  # Avoid comparing the same entry with itself
            similarity = fuzz.ratio(country1, country2)
            if similarity > 80 and similarity < 100:  # Exclude exact matches
                if country1 not in similar_countries_dict:
                    similar_countries_dict[country1] = [country2]
                else:
                    similar_countries_dict[country1].append(country2)

replacements = {
    'TURKS AND CAICOS': 'TURKS  CAICOS',
    'BRITISH OVERSEAS TERRITORY': 'ST HELENA BRITISH OVERSEAS TERRITORY',
    'ST MARTIN': 'ST MAARTIN',
    'UNITED ARAB EMIRATES': 'UNITED ARAB EMIRATES UAE',
    'ANDAMAN  NICOBAR ISLANDAS': 'ANDAMAN ISLANDS',
    'NORTH ATLANTIC OCEAN': 'ATLANTIC OCEAN',
    'MID ATLANTIC OCEAN': 'ATLANTIC OCEAN',
    'SOUTH ATLANTIC OCEAN': 'ATLANTIC OCEAN',
    'SOUTH PACIFIC OCEAN': 'PACIFIC OCEAN',
    'NORTH PACIFIC OCEAN': 'PACIFIC OCEAN',
    'MIDPACIFC OCEAN': 'PACIFIC OCEAN',
    'SOUTHWEST PACIFIC OCEAN': 'PACIFIC OCEAN',
    'TRINIDAD  TOBAGO': 'TRINIDAD',
    'UAE': '',
    'ST HELENA': ''}

shark_attacks.Country = shark_attacks.Country.replace(replacements, regex=True).str.strip()
shark_attacks.Country.sort_values().unique()

# clean State column

shark_attacks.State = shark_attacks.State.str.replace(r'[^a-zA-Z0-9\s]', '', regex=True).str.upper()
shark_attacks.State = shark_attacks.State.str.replace(r'\s+', ' ', regex=True)
shark_attacks.State = shark_attacks.State.str.strip()
shark_attacks.State = shark_attacks.State.replace('nan', 'unknown').str.upper().fillna('Unknown')

shark_attacks.State.sort_values().unique()

# Create a dictionary to store similar state pairs
similar_states_dict = {}

# Get unique states to reduce redundant comparisons/ code was too slow without
unique_states = shark_attacks.State.drop_duplicates()

# Find similar country names and add them to the dictionary
for i, state1 in enumerate(unique_states):
    for j, state2 in enumerate(unique_states):
        if i < j:  # Avoid comparing the same entry with itself
            similarity = fuzz.ratio(state1, state2)
            if similarity > 80 and similarity < 100:  # Exclude exact matches
                if country1 not in similar_states_dict:
                    similar_states_dict[state1] = [state2]
                else:
                    similar_states_dict[state1].append(state2)



reversed__similar_states_dict = {v[0]: k for k, v in similar_states_dict.items()} # complexity

replacements = {'WESTERM AUSTRALIA': 'WESTERN AUSTRALIA',
 'GRAND TERRE': 'GRANDE TERRE',
 'FLORIA': 'FLORIDA',
 'NEW PROVIDENCE ISOAD': 'NEW PROVIDENCE ISLAND',
 'NORTH SOUTH CAROLINA': 'NORTH CAROLINA',
 'NEW SOUTH ALES': 'NEW SOUTH WALES',
 'OUT ISLANDS': 'SOUTH ISLAND',
 'GUERRRERO': 'GUERRERO',
 'NORTHLANDS': 'NORTH ISLAND',
 'QUINTA ROO': 'QUINTANA ROO',
 'SAN ANDRS ISLAND': 'SAN ANDRES ISLAND',
 'LUCAYAN LUCAYAN ARCHIPELAGO': 'LUCAYAN ARCHIPELAGO',
 'AMBERGRIS KEY': 'AMBERGRIS CAYE',
 'GUANACOSTE': 'GUANACASTE',
 'US VIRGIN ISLANDS': 'VIRGIN ISLANDS',
 'VANUA LEVU':  'OFF VANUA LEVU',
 'SAINT GILLES': 'SAINTGILLES',
 'COOK ISLANS': 'COOK ISLANDS',
 'ALAGOAS': 'LAGOS',
 'BAJA CALIFORNIA SUR': 'BAJA CALIFORNIA',
 'SAINT LEU': 'SAINTLEU',
 'TUAMOTUS': 'TUAMOTOS',
 'NORTHERN ORO PROVINCE':  'NORTHERN PROVINCE',
 'INHAMBANE PROVINCE':  'INHAMBE PROVINCE',
 'LETANGSAL': 'LETANGSALE',
 'SAINTBENOT': 'SAINTBENOIT',
 'TOAMASINA PROVINCE': 'ADANA PROVINCE',
 'ANCONA PROVINCE': 'COLN PROVINCE',
 'VITI LEVU': 'VITA LEVU',
 'SINAI PENINSULA': 'SOUTH SINAI PENINSULA',
 'MAKIRAULUWA PROVINCE': 'MAKORAULAWA PROVINCE',
 'SPLITDALMATIA COUNT': 'SPLITDALMATIA COUNTY',
 'BONIN ISLANDS': 'BIMINI ISLANDS',
 'MALAITA PROVINCE': 'MALAMPA PROVINCE',
 'SOUTH CHUNGCHEONG PROVINCE': 'SOUTH CHUNGCHONG PROVINCE',
 'TAVENUI': 'TAVEUNI',
 'PACIFIC COAST': 'NORTH PACIFIC COAST',
 'CAPVERT PENINSULA': 'CAP VERT PENINSULA',
 '5AINTDENIS': 'SAINTDENIS',
 'EHIME PREFECTURE': 'AICHI PREFECTURE',
 'WAKE ISLAND': 'WAKAYA ISLAND',
 'TYRRENIAN SEA': 'TYRRHENIAN SEA',
 'IN THE ENGLISH CHANNEL': 'ENGLISH CHANNEL',
 'EASTERN CAROLINE ISLANDS': 'WESTERN CAROLINE ISLANDS',
 'VERA CRUZ': 'VERACRUZ',
 'LIGUARIA': 'LIGURIA',
 'BAY OF MAPUTU': 'BAY OF MAPUTO',
 'BOUGAINVILLE NORTH SOLOMONS': 'NEAR BOUGAINVILLE NORTH SOLOMONS',
 'OFF THE COAST OF WEST AFRICA': 'OFF COAST OF WEST AFRICA',
 'SOFALA PROVINCE': 'SHEFA PROVINCE',
 'LOMAIVITI PROVINE': 'LOMAIVITI PROVINCE',
 'QUEZON': 'QUEAON',
 'PACIFIC OCEAN': 'SOUTH PACIFIC OCEAN',
 '1000 MILES WEST OF HAWAII': '1000 MILES EAST OF HAWAII',
 'ABAU SUB DISTRICT CENTRAL PROVINCE': 'ABAU SUBDISTRICTCENTRAL PROVINCE',
 'SOUTH KOREA': 'SOUTH SHORE',
 'SHATALARAB RIVER': 'SHATTALARAB RIVER',
 'CALVADOS ARCHIPELAGO': 'CYCLADES ARCHIPELAGO',
 'GALICA':  'GALICIA',
 'OFF CAPE HAITIEN': 'CAPE HAITIEN',
 'MLAGA': 'MALAGA',
 'OFF THE COAST OF SOUTH AMERICA': 'OFF THE COAST OF WEST AFRICA'}


shark_attacks.State = shark_attacks.State.replace(replacements, regex=True).str.strip()
shark_attacks.State.unique()

# clean activity columns

shark_attacks.Activity_Original = shark_attacks.Activity

shark_attacks.Activity_Cleaned = (shark_attacks.Activity_Original.str.replace(r'[^a-zA-Z0-9\s]', '', regex=True).str.lower().str.strip().fillna('Unknown'))

shark_attacks.Activity_Cleaned = shark_attacks.Activity_Cleaned.replace('', 'Unknown')

replacements = {
    'Crew swimming alongside their anchored ship': 'swimming around anchored ship',
    'swmming': 'swimming',
    'oveboard': 'overboard',
    'boat of a hawaiian brig was stove in by  whale': 'boat sinking',
    'surfng': 'surfing',
    'fihing': 'fishing',
    'snorkelling': 'snorkeling',
 'foilboarding': 'foil boarding',
 'paddleboarding': 'paddle boarding',
 'boggie boarding': 'boogie boarding',
 'surfskiing': 'surf skiing',
 'harassing a shark': 'harassing sharks',
 'standup paddleboarding': 'stand up paddleboarding',
 'bodyboarding': 'body boarding',
 'standup paddle boarding': 'standup paddleboarding',
 'kiteboarding': 'kite boarding',
 'airsea diasaster': 'airsea disaster',
 'removing shark from a net': 'removing shark from net',
 'killing a shark': 'killing sharks',
 'paddleskiing': 'paddle skiing',
 'standing on sandbar': 'standing on sandbank',
    'standup paddle boarding':'standup paddle boarding',
 'stand up paddle boarding': 'standup paddle boarding',
 'standup paddleboarding': 'standup paddle boarding',
 'airsea diasaster':  'airsea disaster',
 'killing  sharks': 'killing sharks',
    'stand up paddle boarding': 'standup paddle boarding'}

shark_attacks.Activity_Cleaned = shark_attacks.Activity_Cleaned.replace(replacements, regex=True)

activity_mapping = {
    'wreck': 'wreck',
    'washing': 'washing item/animal',
    'diving': 'diving',
    'swimming': 'swimming',
    'bathing': 'bathing',
    'surfing': 'surfing',
    'paddling': 'paddling',
    'rescuing': 'rescuing',
    'floating': 'floating',
    'plane|aircraft': 'aircraft',
    'sank|sinking|capsized': 'boat sinking',
    'shark.*fishing|fishing.*shark': 'fishing shark',
    'fishing for': 'fishing fish other than shark',
    'fishing': 'fishing',
    'fell': 'fell',
    'jumped': 'jumped',
    'adrift': 'adrift',
    'wearing life jackets': 'adrift',
    'sunk': 'sunk',
    'abandoning': 'abandoning',
    'airsea diasaster': 'air disaster'}

# Apply regex-based categorization
for pattern, replacement in activity_mapping.items():
    shark_attacks.loc[
        shark_attacks.Activity_Cleaned.str.contains(pattern, case=False, na=False), 'Activity_Cleaned'
    ] = replacement

# Create a dictionary to store similar state pairs
similar_Activity_Cleaned_dict = {}

# Get unique states to reduce redundant comparisons/ code was too slow without
unique_Activity_Cleaned = shark_attacks.Activity_Cleaned.drop_duplicates()

# Find similar country names and add them to the dictionary
for i, act1 in enumerate(unique_Activity_Cleaned):
    for j, act2 in enumerate(unique_Activity_Cleaned):
        if i < j:  # Avoid comparing the same entry with itself
            similarity = fuzz.ratio(act1, act2)
            if similarity > 90 and similarity < 100:  # Exclude exact matches
                if act1 not in similar_Activity_Cleaned_dict:
                    similar_Activity_Cleaned_dict[act1] = [act2]
                else:
                    similar_Activity_Cleaned_dict[act1].append(act2)

similar_Activity_Cleaned_dict
reversed_similar_Activity_dict = {v[0]: k for k, v in similar_Activity_Cleaned_dict.items()}
reversed_similar_Activity_dict
shark_attacks.Activity_Cleaned.unique()

# Clean and convert age to numeric value, keeping specific categories

def clean_age(age):

    # Handle case where age is already a numeric value
    if isinstance(age, (int, float)):
        return age

    # Handle case where age is a string
    if isinstance(age, str):
        age = age.lower().strip()

        # Handle specific categories
        if age in ['middle age', 'young', 'teen', 'minor', 'adult', 'elderly','toddler', 'child', 'kid']:
            return age
        elif age in ['x', 'make line green', 'f', 'm', 'a.m.']:
            return np.nan
        elif 'months' in age or 'month' in age:  # Handle age in months
            return 'baby'
        elif age.startswith('mid-') and re.match(r'\d+', age.split('-')[1]):
            return int(age.split('-')[1][0])  # Extracts the base number of the 'mid-20s' case (returns 20)
        elif age.startswith('ca.') and re.match(r'\d+', age[3:].strip()):  # Adjusted to correctly handle 'ca. 33'
            return int(re.findall(r'\d+', age[3:].strip())[0])  # Extracts 'ca. 33' to 33
        elif re.match(r'^\d+$', age):  # Handle simple numeric ages
            return int(age)
        elif re.match(r'^\d+\s?[+-]?$', age):  # Handle ages like '60+' or '40-'
            return int(re.findall(r'\d+', age)[0])
        elif re.match(r'\d+', age):  # Handle ranges or mixed ages like '30s', '20/30'
            return int(re.findall(r'\d+', age)[0])
        elif re.search(r'both\s\d+', age):  # Handle "both 11", "both 9", etc.
            return int(re.findall(r'\d+', age)[0])
        elif 'to' in age:  # Handle ranges like '20 to 30'
            numbers = re.findall(r'\d+', age)
            return np.mean([int(num) for num in numbers])
        elif 'and' in age:  # Handle age lists like '28 & 22'
            numbers = re.findall(r'\d+', age)
            return np.mean([int(num) for num in numbers])
        elif re.match(r'[a-z]+', age):  # Handle known string categories
            return age
        else:
            return np.nan  # Return NaN for unknown or invalid values

    # Handle cases where age is not valid
    return np.nan

# Assuming shark_attacks is your DataFrame, apply the clean_age function to the Age column
shark_attacks['Age'] = shark_attacks['Age'].apply(clean_age)



#Create a new column 'AgeGroup'. This column should categorize customers as:
#'baby' (age<=2), 'child' (age <= 12), 'teen' (12 < age <=18), 'young' (19 <= age <= 39) 'middle age' (40 < age <= 59), and 'senior' (age > 60)

def categorize_age(age):

    if isinstance(age, (int, float)):  # Numeric ages
        if age <= 2:
            return 'baby'
        elif age <= 12:
            return 'child'
        elif 12 < age <= 18:
            return 'teen'
        elif 19 <= age <= 39:
            return 'young'
        elif 40 <= age <= 59:
            return 'middle age'
        elif age >= 60:
            return 'senior'
        else:
            return None

    elif isinstance(age, str):  # Handling categorical string values
        age = age.lower().strip()
        if age in ['baby']:
            return 'baby'
        elif age in ['child', 'kid', 'a minor']:
            return 'child'
        elif age in ['teen', 'teens']:
            return 'teen'
        elif age in ['young']:
            return 'young'
        elif age in ['middle age']:
            return 'middle age'
        elif age in ['elderly', 'senior']:
            return 'senior'
        else:
            return None  # For unknown cases

    return None  # Handle NaN or other invalid values

# Apply the function to create 'AgeGroup' column
shark_attacks['AgeGroup'] = shark_attacks['Age'].apply(categorize_age)

# Check the values in 'AgeGroup'
print(shark_attacks['AgeGroup'].value_counts())
print("Null values:",shark_attacks['AgeGroup'].isna().sum())

#check with group the strategy to fill empty rows

shark_attacks.Age.unique()

# check different most common words for Injury column

from collections import Counter
import pandas as pd

# Remove NaN values and join all text in the column
all_text = " ".join(shark_attacks['Injury'].dropna()).lower()

# Split text into words
word_list = all_text.split()

# Count word occurrences
word_counts = Counter(word_list)

# Display the 100 most common words
print(word_counts.most_common(100))

# Define function to categorize injuries:

# Find rows where 'Injury' contains 'no injury' or 'no injuries'
no_injury_cases = shark_attacks[shark_attacks['Injury'].str.contains(r'no\s*i[nj]{1,2}uri?e?s?', case=False, na=False, regex=True)]

# minor injuries: superficial wounds, minor lacerations;
# moderate injuries: deep lacerations, puncture wounds, tissue loss;
# severe injuries: life-threating wounds, amputations, major tissue loss;
# fatal injuries: drowning, death, organ damage

def categorize_injury(injury):
    if pd.isna(injury):  # Handle missing values
        return np.nan

    injury = injury.lower()  # Convert to lowercase for consistent matching

    # Check for 'no injury' cases first
    if any(word in injury for word in ['no injury', 'no injuries', 'no inujuri', 'no ijnury','no inury','no attack']):
        return 'no injury'

    # Define injury groups based on keywords
    if  any(word in injury for word in ['fatal','disappeared','death','drowned','drowning','killed']):
        return 'fatal injuries'
    elif any(word in injury for word in ['severe','severed','severely', 'severly','severted','serious','significant','amputated','lost','cut','cuts','tissue loss','life threatening','not recovered', 'inch','cm']):
        return 'severe injuries'
    elif any(word in injury for word in ['lacerations','laceration','lacerated','moderate', 'puncture','punctures', 'non-life','bite','bitten', 'leg injured','leg injury', 'hand']):
        return 'moderate injuries'
    elif any(word in injury for word in ['minor','small','abrasion','abrasions','bruised','bruises','superficial','shallow']):
        return 'minor injuries'
    else:
        return 'other/unknown'  # Default category if no match is found

# Apply function to create the 'InjuryGroup' column
shark_attacks['InjuryGroup'] = shark_attacks['Injury'].apply(categorize_injury).fillna('other/unknown')

# Check the values in the InjuryGroup
print(shark_attacks['InjuryGroup'].value_counts())
print("Null values:",shark_attacks['InjuryGroup'].isna().sum())

# standardize Fatal column to N and Y
shark_attacks['FatalGroup Y/N'] = shark_attacks['Fatal Y/N'].replace({'F': np.nan,'M': np.nan, np.nan: np.nan,'UNKNOWN': np.nan, 2017: np.nan, 'Y': 'Y','Y x 2': 'Y','y': 'Y'})
shark_attacks['FatalGroup Y/N'] = shark_attacks['FatalGroup Y/N'].replace(to_replace=r'^\s*N\s*$', value='N', regex=True)
shark_attacks['FatalGroup Y/N'] = shark_attacks['FatalGroup Y/N'].fillna('N').replace({'n': 'N','Nq': 'N',' N': 'N'})

# Check the unique values in 'FatalGroup'
print(shark_attacks['FatalGroup Y/N'].value_counts())
print("Null values:",shark_attacks['FatalGroup Y/N'].isna().sum())

# Iryna added
print(shark_attacks[['Sex', 'Date', 'Location']].head())
# Replaced empty strings and whitespace-only strings with NaN
shark_attacks['Sex'].replace(r'^\s*$', np.nan, regex=True, inplace=True)
shark_attacks['Date'].replace(r'^\s*$', np.nan, regex=True, inplace=True)
shark_attacks['Location'].replace(r'^\s*$', np.nan, regex=True, inplace=True)
# Sex Column
# Striped whitespace and convert to uppercase
shark_attacks['Sex'] = shark_attacks['Sex'].str.strip().str.upper()
# Retained only valid entries ('M' or 'F'), set others to NaN
shark_attacks['Sex'] = shark_attacks['Sex'].apply(lambda x: x if x in ['M', 'F'] else np.nan)
# Date Column
# Converted to datetime, coerce errors to NaT
shark_attacks['Date'] = pd.to_datetime(shark_attacks['Date'], errors='coerce')
#Location Column
# Striped leading/trailing whitespace and standardize case
shark_attacks['Location'] = shark_attacks['Location'].str.strip().str.title()
# Removed entries with excessive length or non-alphabetic characters
shark_attacks['Location'] = shark_attacks['Location'].apply(lambda x: x if pd.notnull(x) and len(x) < 100 and x.isalpha() else np.nan)
# Droped duplicate rows based on 'Sex', 'Date', and 'Location'
shark_attacks.drop_duplicates(subset=['Sex', 'Date', 'Location'], inplace=True)
# Displayed cleaned data overview
print("\nCleaned Data Overview:")
print(shark_attacks[['Sex', 'Date', 'Location']].head())

#Remove the coma and the zero in the Year column

shark_attacks['Year'] = shark_attacks['Year'].fillna(0).astype(int).astype(str)

print(shark_attacks)

#Listing the unique values in Type column

shark_attacks['Type'].unique()

unique_counts = shark_attacks.groupby('Type').nunique()

print(unique_counts)

#Rename several types into one type

shark_attacks['Type'] = shark_attacks['Type'].replace(['Unconfirmed', 'Under investigation', 'Unverified', 'Invalid','?'], 'Unknown')
shark_attacks['Type'] = shark_attacks['Type'].replace(['Sea Disaster', 'Watercraft', 'Boat'], 'Unknown')
shark_attacks['Type'].unique()

#Remove blank space of ' Unprovoked'

shark_attacks['Type'] = shark_attacks['Type'].str.lstrip()
shark_attacks['Type'].unique()

#Cleaning species :

#STEP 1 : Remove blank space :
shark_attacks.columns = shark_attacks.columns.str.strip()

#STEP 2 : How many unique values ?
shark_attacks['Species'].nunique() #1706 unique values

#STEP 3 : Standardize all variations of "tiger shark" into a single value "Tiger shark" using replace() with Regex (Flexible for Multiple Variations)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*tiger shark.*', 'Tiger shark', regex=True)

#do the same for the other species
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*white shark.*', 'White shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*bull shark.*', 'Bull shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*whitetip shark.*', 'Whitetip shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*whaler shark.*', 'Whaler shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*small shark.*', 'Small shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*large shark.*', 'Large shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*Zambesi shark.*', 'Zambesi shark', regex=True)
shark_attacks['Species'] = shark_attacks['Species'].replace(r'(?i).*great white.*', 'White shark', regex=True)

#STEP 4 : Check the unique values
shark_attacks['Species'].nunique()

shark_attacks.columns.tolist()

# Hypothesis 1

# If there is correlation between the type of species with the number of fatal cases - exploring whether the type of the shark can predict the fatality of the incident.

pivot_table = shark_attacks.pivot_table(index='Species', columns='FatalGroup Y/N', aggfunc='size', fill_value=0)
pivot_table

# conclusions about species

# Hypothesis 2
# Higher number of shark attacks based on the location - in which locations we notice higher number of incidents.
# assuming each row one incident

shark_attacks.groupby('Country')['FatalGroup Y/N'].agg(['count']).sort_values(by='count',ascending=False).head(10).plot(kind='bar', legend=False, color='royalblue')

#pivot_table_location = shark_attacks.pivot_table(index=['Country'], columns='FatalGroup Y/N', aggfunc='size', fill_value=0)
#pivot_table_location

shark_attacks.groupby('Country')['FatalGroup Y/N'].agg(['count']).sort_values(by='count',ascending=False).head(10)

# Hypothesis 3 - We expect higher number of shark attacks in summer months - to test this hypothesis whether is true

shark_attacks['Month'] = shark_attacks['Date'].dt.month

shark_attacks.groupby('Month')['FatalGroup Y/N'].agg(['count']).sort_values(by='Month').plot(kind='bar', legend=False)

# We notice as expected hifgher number of cases in summer months. But we also notice that Jnauary has sthe highest number.

grouped_data = shark_attacks.groupby(['Country','Month'])['FatalGroup Y/N'].agg(['count']).sort_values(by='count')

filtered_data = grouped_data[grouped_data.index.get_level_values('Month') == 1].sort_values(by='count')
print(filtered_data)

#conclusion that Australia, ZA, USA, NZ affect the data

# Hypothesis 4 - How the type of activity can predict the possibility for a fatal incident - the correlation between activity and number of fatal incidents

shark_attacks.head()

shark_attacks_filtered = shark_attacks[shark_attacks['FatalGroup Y/N'] == 'Y']

pivot_table1 = shark_attacks_filtered.pivot_table(index='Activity_Cleaned', columns='Type', values='FatalGroup Y/N', aggfunc='size', fill_value=0)
pivot_table1

top_5 = pivot_table1.sum(axis=1).nlargest(5)
top_5_df = pivot_table1.loc[top_5.index]

top_5.drop(columns='Total').plot(kind='bar', stacked=True, figsize=(10, 6))

shark_attacks.Activity_Cleaned.isna()
shark_attacks.head()